# ==============================================================================
# TASK 3: VOICE TRANSCRIPTION RECON
# ==============================================================================
# Mission: Map voice infrastructure and identify transcription integration pattern

voice_recon:
  backend:
    venom_voice_purpose: |
      VenomVoice is NOT for audio/voice - it's the TEXT response formatting layer

      Location: core/venom_voice.py (41KB file)
      Purpose: System prompt construction and response parsing
      Role in architecture:
        - Injects brain state into LLM API calls (system prompt)
        - Parses LLM output for action markers ([REMEMBER], [REFLECT], etc.)
        - Handles streaming response formatting
        - Applies "voice style" (corporate vs troll personality)

      Quote from docstring (L3-25):
        "NOT an agent. NOT a separate system. Just HOW the unified CognitiveTwin
         brain speaks through the lobotomized API."

      Key point: This is about TEXT tone/style, NOT audio transcription

    tts_exists: false

    stt_exists: false

    audio_endpoints: |
      NONE found in codebase

      Evidence:
      - No /api/audio/* routes in main.py
      - No audio upload handling in main.py
      - No file upload endpoints (multipart/form-data)
      - WebSocket only handles JSON messages (no binary frames)

    speech_service: "NONE"

    dependencies_checked:
      location: "Would be in requirements.txt or imports"
      findings: |
        No speech SDKs found:
        - No "whisper" imports
        - No "deepgram" imports
        - No "azure.cognitiveservices.speech" imports
        - No "openai.audio" imports
        - No "google.cloud.speech" imports

      grep_results: "0 files matched speech service patterns"

    voice_config: |
      Location: core/config.yaml:L88-110

      Settings control TEXT personality, not audio:
      ```yaml
      voice:
        engine: venom              # Text response engine
        style: corporate           # corporate | helpful | troll
        company_name: "Driscoll Foods"
        sign_off: false
        default: corporate

        templates:
          corporate:
            personality: professional
            snark_level: 0

          troll:
            personality: sarcastic
            snark_level: 7

        division_voice:
          sales: corporate
          transportation: troll
          operations: corporate
          default: corporate
      ```

      This configures response TONE, not audio processing

  frontend:
    audio_capture: "NONE"

    microphone_ui: "NONE"

    evidence: |
      No audio-related components found:
      - No microphone SVG icons
      - No MediaRecorder API usage
      - No getUserMedia() calls
      - No audio element playback
      - No Web Audio API usage

      Frontend is pure text chat interface

  integration_pattern:
    current_flow: |
      Text only:
      1. User types message in textarea
      2. Frontend sends {"type": "message", "content": "text"}
      3. Backend processes via EnterpriseTwin.think_streaming()
      4. Backend streams text chunks back
      5. Frontend displays in chat

    proposed_voice_flow: |
      Add voice input path:

      Frontend:
      1. User clicks microphone button
      2. Browser captures audio via getUserMedia()
      3. Audio recorded in chunks (MediaRecorder)
      4. On stop, send audio blob to backend
      5. Backend transcribes to text
      6. Backend processes text via normal message flow
      7. Response streamed back as text
      8. Optional: Convert response to speech (TTS)

      Backend:
      1. Add POST /api/transcribe endpoint
      2. Accept audio/* content-type
      3. Send to transcription service (Whisper/Deepgram/Azure)
      4. Return {"text": "transcribed text"}
      5. Frontend injects into normal message flow

    alternative_realtime_flow: |
      WebSocket binary frames:

      1. Upgrade WebSocket to accept binary messages
      2. Stream audio chunks in real-time
      3. Backend sends to streaming transcription service
      4. Return partial transcripts as they arrive
      5. Finalize on silence detection or manual stop

      More complex but better UX for long voice messages

  where_transcription_plugs_in:
    option_1_rest_endpoint:
      location: "core/main.py after L614 (near /api/content)"
      code: |
        ```python
        from pydantic import BaseModel
        import base64

        class TranscribeRequest(BaseModel):
            audio_base64: str
            format: str = "webm"  # webm, mp3, wav
            language: str = "en"

        @app.post("/api/transcribe")
        async def transcribe_audio(
            request: TranscribeRequest,
            user: dict = Depends(require_auth)
        ):
            """Transcribe audio to text."""
            try:
                # Decode base64 audio
                audio_bytes = base64.b64decode(request.audio_base64)

                # Send to transcription service
                transcription_service = get_transcription_service()
                text = await transcription_service.transcribe(
                    audio_bytes,
                    format=request.format,
                    language=request.language
                )

                return {
                    "success": true,
                    "text": text,
                    "language": request.language,
                    "duration_ms": len(audio_bytes) // 16  # Estimate
                }
            except Exception as e:
                raise HTTPException(500, f"Transcription failed: {e}")
        ```

    option_2_websocket_binary:
      location: "core/main.py:L683 (websocket_endpoint)"
      modification: |
        Add binary message handling:

        ```python
        while True:
            # Current: data = await websocket.receive_json()
            # New: Accept both text and binary

            message = await websocket.receive()

            if "text" in message:
                # Existing JSON message handling
                data = json.loads(message["text"])
                msg_type = data.get("type", "message")
                # ... existing logic

            elif "bytes" in message:
                # New: Handle audio chunks
                audio_chunk = message["bytes"]

                # Send to streaming transcription
                transcription_service = get_transcription_service()
                partial_text = await transcription_service.transcribe_chunk(
                    audio_chunk,
                    session_id=session_id
                )

                # Send partial transcript
                await websocket.send_json({
                    "type": "partial_transcript",
                    "text": partial_text,
                    "is_final": false
                })
        ```

    option_3_multipart_upload:
      location: "core/main.py after L614"
      code: |
        ```python
        from fastapi import File, UploadFile

        @app.post("/api/transcribe/upload")
        async def transcribe_upload(
            file: UploadFile = File(...),
            language: str = "en",
            user: dict = Depends(require_auth)
        ):
            """Transcribe uploaded audio file."""
            # Read file
            audio_bytes = await file.read()

            # Validate file type
            if not file.content_type.startswith("audio/"):
                raise HTTPException(400, "File must be audio")

            # Send to transcription service
            transcription_service = get_transcription_service()
            text = await transcription_service.transcribe(
                audio_bytes,
                format=file.content_type.split("/")[1],
                language=language
            )

            return {
                "success": true,
                "text": text,
                "filename": file.filename
            }
        ```

  how_transcripts_become_memories:
    current_memory_flow: |
      Text messages already stored in ChatMemoryStore:

      Location: memory/chat_memory.py
      Method: record_exchange(session_id, user_query, model_response, ...)
      Storage: data/chat_exchanges/exchange_{id}.json

    voice_integration: |
      Transcripts flow through SAME path as text:

      1. Audio captured by frontend
      2. Sent to /api/transcribe -> text
      3. Frontend calls session.sendMessage(transcribed_text)
      4. WebSocket processes as normal text message
      5. EnterpriseTwin.think_streaming() responds
      6. ChatMemoryStore.record_exchange() saves like any other message

      Key point: Transcription is PRE-PROCESSING
      After transcription, it's just text through normal pipeline

    metadata_preservation: |
      Track that message originated from voice:

      In record_exchange (chat_memory.py:L129-181):
      Add metadata field:
      ```python
      def record_exchange(
          self,
          session_id: str,
          user_query: str,
          model_response: str,
          input_method: str = "text",  # NEW: "text" | "voice"
          **kwargs
      ):
          # ... existing code
          exchange = ChatExchange(
              # ... existing fields
              metadata={
                  "input_method": input_method,
                  "audio_duration_ms": kwargs.get("audio_duration_ms"),
                  **kwargs.get("metadata", {})
              }
          )
      ```

      Then in main.py WebSocket handler, pass input_method="voice"

  transcription_service_implementation:
    create_service_file:
      location: "core/transcription_service.py (NEW)"
      class_structure: |
        ```python
        from abc import ABC, abstractmethod
        from typing import Optional
        import os
        import asyncio

        class TranscriptionService(ABC):
            @abstractmethod
            async def transcribe(
                self,
                audio_bytes: bytes,
                format: str = "webm",
                language: str = "en"
            ) -> str:
                """Transcribe audio to text."""
                pass

        class WhisperTranscriptionService(TranscriptionService):
            """OpenAI Whisper API transcription."""

            def __init__(self, api_key: str):
                self.api_key = api_key
                self.endpoint = "https://api.openai.com/v1/audio/transcriptions"

            async def transcribe(
                self,
                audio_bytes: bytes,
                format: str = "webm",
                language: str = "en"
            ) -> str:
                import aiohttp

                # Prepare multipart form data
                form = aiohttp.FormData()
                form.add_field("file", audio_bytes, filename=f"audio.{format}", content_type=f"audio/{format}")
                form.add_field("model", "whisper-1")
                form.add_field("language", language)

                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        self.endpoint,
                        headers={"Authorization": f"Bearer {self.api_key}"},
                        data=form
                    ) as resp:
                        if resp.status != 200:
                            raise Exception(f"Whisper API error: {resp.status}")

                        result = await resp.json()
                        return result["text"]

        class DeepgramTranscriptionService(TranscriptionService):
            """Deepgram transcription (faster, cheaper than Whisper)."""

            def __init__(self, api_key: str):
                self.api_key = api_key
                self.endpoint = "https://api.deepgram.com/v1/listen"

            async def transcribe(
                self,
                audio_bytes: bytes,
                format: str = "webm",
                language: str = "en"
            ) -> str:
                import aiohttp

                async with aiohttp.ClientSession() as session:
                    async with session.post(
                        f"{self.endpoint}?punctuate=true&language={language}",
                        headers={
                            "Authorization": f"Token {self.api_key}",
                            "Content-Type": f"audio/{format}"
                        },
                        data=audio_bytes
                    ) as resp:
                        if resp.status != 200:
                            raise Exception(f"Deepgram API error: {resp.status}")

                        result = await resp.json()
                        return result["results"]["channels"][0]["alternatives"][0]["transcript"]

        class AzureSpeechTranscriptionService(TranscriptionService):
            """Azure Cognitive Services Speech-to-Text."""

            def __init__(self, subscription_key: str, region: str):
                self.subscription_key = subscription_key
                self.region = region

            async def transcribe(
                self,
                audio_bytes: bytes,
                format: str = "webm",
                language: str = "en"
            ) -> str:
                # Azure SDK is synchronous, run in executor
                loop = asyncio.get_event_loop()
                return await loop.run_in_executor(
                    None,
                    self._transcribe_sync,
                    audio_bytes,
                    format,
                    language
                )

            def _transcribe_sync(self, audio_bytes: bytes, format: str, language: str) -> str:
                import azure.cognitiveservices.speech as speechsdk

                # Create speech config
                speech_config = speechsdk.SpeechConfig(
                    subscription=self.subscription_key,
                    region=self.region
                )
                speech_config.speech_recognition_language = language

                # Create audio config from bytes
                audio_stream = speechsdk.audio.PushAudioInputStream()
                audio_stream.write(audio_bytes)
                audio_stream.close()
                audio_config = speechsdk.audio.AudioConfig(stream=audio_stream)

                # Create recognizer
                speech_recognizer = speechsdk.SpeechRecognizer(
                    speech_config=speech_config,
                    audio_config=audio_config
                )

                # Recognize
                result = speech_recognizer.recognize_once()

                if result.reason == speechsdk.ResultReason.RecognizedSpeech:
                    return result.text
                else:
                    raise Exception(f"Azure Speech error: {result.reason}")

        # Singleton
        _transcription_service: Optional[TranscriptionService] = None

        def get_transcription_service() -> TranscriptionService:
            global _transcription_service
            if _transcription_service is None:
                # Read from config
                provider = os.getenv("TRANSCRIPTION_PROVIDER", "whisper")

                if provider == "whisper":
                    api_key = os.getenv("OPENAI_API_KEY")
                    if not api_key:
                        raise ValueError("OPENAI_API_KEY not set")
                    _transcription_service = WhisperTranscriptionService(api_key)

                elif provider == "deepgram":
                    api_key = os.getenv("DEEPGRAM_API_KEY")
                    if not api_key:
                        raise ValueError("DEEPGRAM_API_KEY not set")
                    _transcription_service = DeepgramTranscriptionService(api_key)

                elif provider == "azure":
                    key = os.getenv("AZURE_SPEECH_KEY")
                    region = os.getenv("AZURE_SPEECH_REGION")
                    if not key or not region:
                        raise ValueError("AZURE_SPEECH_KEY and AZURE_SPEECH_REGION must be set")
                    _transcription_service = AzureSpeechTranscriptionService(key, region)

                else:
                    raise ValueError(f"Unknown transcription provider: {provider}")

            return _transcription_service
        ```

  frontend_implementation:
    create_audio_capture_component:
      location: "frontend/src/lib/components/VoiceInput.svelte (NEW)"
      code: |
        ```svelte
        <script lang="ts">
            import { createEventDispatcher } from 'svelte';

            const dispatch = createEventDispatcher();

            let recording = false;
            let mediaRecorder: MediaRecorder | null = null;
            let audioChunks: Blob[] = [];

            async function startRecording() {
                try {
                    const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
                    mediaRecorder = new MediaRecorder(stream);

                    audioChunks = [];

                    mediaRecorder.ondataavailable = (event) => {
                        audioChunks.push(event.data);
                    };

                    mediaRecorder.onstop = async () => {
                        const audioBlob = new Blob(audioChunks, { type: 'audio/webm' });
                        await sendToTranscription(audioBlob);

                        // Stop all tracks
                        stream.getTracks().forEach(track => track.stop());
                    };

                    mediaRecorder.start();
                    recording = true;
                } catch (err) {
                    console.error('Failed to start recording:', err);
                    alert('Microphone access denied');
                }
            }

            function stopRecording() {
                if (mediaRecorder && recording) {
                    mediaRecorder.stop();
                    recording = false;
                }
            }

            async function sendToTranscription(audioBlob: Blob) {
                try {
                    // Convert to base64
                    const reader = new FileReader();
                    reader.readAsDataURL(audioBlob);
                    reader.onloadend = async () => {
                        const base64Audio = (reader.result as string).split(',')[1];

                        // Send to backend
                        const apiBase = import.meta.env.VITE_API_URL || 'http://localhost:8000';
                        const response = await fetch(`${apiBase}/api/transcribe`, {
                            method: 'POST',
                            headers: {
                                'Content-Type': 'application/json',
                                'X-User-Email': localStorage.getItem('user_email') || ''
                            },
                            body: JSON.stringify({
                                audio_base64: base64Audio,
                                format: 'webm',
                                language: 'en'
                            })
                        });

                        if (!response.ok) {
                            throw new Error('Transcription failed');
                        }

                        const data = await response.json();

                        // Emit transcribed text
                        dispatch('transcribed', { text: data.text });
                    };
                } catch (err) {
                    console.error('Transcription error:', err);
                    alert('Failed to transcribe audio');
                }
            }

            function handleClick() {
                if (recording) {
                    stopRecording();
                } else {
                    startRecording();
                }
            }
        </script>

        <button
            class="voice-button"
            class:recording
            on:click={handleClick}
            title={recording ? 'Stop recording' : 'Start recording'}
        >
            {#if recording}
                <svg><!-- Stop icon --></svg>
                <span class="pulse"></span>
            {:else}
                <svg><!-- Microphone icon --></svg>
            {/if}
        </button>

        <style>
            .voice-button {
                /* Styling */
            }

            .voice-button.recording {
                animation: pulse 1s infinite;
            }

            @keyframes pulse {
                0%, 100% { opacity: 1; }
                50% { opacity: 0.6; }
            }
        </style>
        ```

    integrate_into_chat:
      location: "frontend/src/routes/chat/+page.svelte (or wherever chat UI is)"
      modification: |
        ```svelte
        <script>
            import VoiceInput from '$lib/components/VoiceInput.svelte';

            function handleTranscribed(event) {
                const text = event.detail.text;
                // Send as normal message
                session.sendMessage(text);
            }
        </script>

        <div class="input-container">
            <textarea bind:value={$session.inputValue} />
            <button on:click={() => session.sendMessage()}>Send</button>
            <VoiceInput on:transcribed={handleTranscribed} />
        </div>
        ```

  decision_needed:
    transcription_provider: |
      Options:
      1. OpenAI Whisper ($0.006/minute)
         - Pros: Best accuracy, multilingual
         - Cons: Slower (5-10s for 1min audio)

      2. Deepgram ($0.0043/minute)
         - Pros: Faster (1-2s for 1min), cheaper
         - Cons: Requires API key management

      3. Azure Speech ($1/hour = $0.0167/minute)
         - Pros: Enterprise features, compliance
         - Cons: Most expensive, complex setup

      4. AssemblyAI ($0.00025/second = $0.015/minute)
         - Pros: Good balance of speed/accuracy
         - Cons: Another vendor to manage

      Recommendation: Start with Whisper (easiest), migrate to Deepgram if speed matters

    real_time_vs_batch: |
      Batch (recommended for MVP):
      - User records full message
      - Click stop -> send for transcription
      - Wait 2-5 seconds
      - Display transcribed text
      - User confirms/edits
      - Send to bot

      Real-time (future enhancement):
      - Streaming audio chunks
      - Partial transcripts appear as user speaks
      - More complex but better UX

  estimated_complexity:
    backend_effort: |
      - TranscriptionService class: 3-4 hours
      - POST /api/transcribe endpoint: 1-2 hours
      - Error handling + validation: 1 hour
      - Testing with real audio: 2 hours
      Total: 7-9 hours

    frontend_effort: |
      - VoiceInput component: 3-4 hours
      - MediaRecorder integration: 2 hours
      - UI/UX polish: 2 hours
      - Browser compatibility testing: 1 hour
      Total: 8-9 hours

    provider_setup: |
      - Sign up for service: 0.5 hour
      - Get API keys: 0.5 hour
      - Configure environment: 0.5 hour
      - Test transcription quality: 1 hour
      Total: 2.5 hours

    total_mvp: "17-20 hours for working voice input"

  dependencies:
    python_packages: |
      For Whisper:
      - aiohttp (likely already installed)

      For Deepgram:
      - aiohttp

      For Azure:
      - azure-cognitiveservices-speech

      Add to requirements.txt:
      ```
      aiohttp==3.9.1
      # azure-cognitiveservices-speech==1.35.0  # Uncomment if using Azure
      ```

    environment_variables: |
      Add to .env:
      ```
      TRANSCRIPTION_PROVIDER=whisper  # whisper | deepgram | azure
      OPENAI_API_KEY=sk-...  # If using Whisper
      DEEPGRAM_API_KEY=...   # If using Deepgram
      AZURE_SPEECH_KEY=...   # If using Azure
      AZURE_SPEECH_REGION=eastus  # If using Azure
      ```

  blockers:
    none_technical: |
      All patterns exist. Standard REST + WebSocket architecture supports this.

    product_decisions_needed:
      - Which transcription provider?
      - Support multiple languages or English only?
      - Allow voice in all departments or just some?
      - Store audio files or just transcripts?
      - TTS for responses (read back) or text only?

  future_enhancements:
    tts_for_responses:
      description: "Read bot responses aloud"
      complexity: "Similar to STT but reversed"
      providers: "OpenAI TTS, ElevenLabs, Azure Speech"
      effort: "6-8 hours"

    speaker_diarization:
      description: "Identify different speakers in multi-person audio"
      use_case: "Meeting transcription"
      providers: "Deepgram, AssemblyAI"
      effort: "4-6 hours"

    custom_vocabulary:
      description: "Train on company-specific terms (product names, jargon)"
      providers: "Deepgram Custom Vocabulary, Azure Custom Speech"
      effort: "8-12 hours (requires training data)"

    voice_commands:
      description: "Shortcuts like 'switch to sales' or 'send message'"
      pattern: "Post-transcription NLU parsing"
      effort: "4-6 hours"

no_code_changes_needed: |
  Voice transcription is a NEW feature, not a modification.
  No existing code conflicts. Clean additive implementation.
